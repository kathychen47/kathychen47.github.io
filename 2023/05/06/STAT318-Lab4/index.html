<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Kathy Chen's blog</title><meta name="description" content="Kathy's learning notes"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Good morning everyone, and welcome back to Lab class! I hope you all had a great break and are feeling refreshed and ready to dive into today’s lesson. Today, we’ll be covering the validation set approach, k-fold cross validation and Bootstrap method. These three method are both used to evaluate performance of the model. 
I will introduce you the validatio.."><script src="//unpkg.com/valine/dist/Valine.min.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Kathy Chen's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center"></p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile is-hidden"></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle"></h1><time class="has-text-grey" datetime="2023-05-06T05:27:20.708Z">2023-05-06</time><article class="mt-2 post-content"><p>Good morning everyone, and welcome back to Lab class! I hope you all had a great break and are feeling refreshed and ready to dive into today’s lesson. Today, we’ll be covering the validation set approach, k-fold cross validation and Bootstrap method. These three method are both used to evaluate performance of the model. </p>
<p>I will introduce you the validation set approach first. For the validation set approach, the data is split into two parts: a training set and a validation set. The model is trained on the training set and the validation set is used to estimate its performance. </p>
<p> First of all, Let’s see what the dataset looks like, just like we do every time we get a new dataset. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">library(ISLR)
set.seed(6678)
data(Auto)
attach(Auto)
dim(Auto)
names(Auto)
Auto[1:4,]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>We will continue using Auto dataset. We load the library ISLR for the use of Auto dataset and ggplots library for ploting. We set the seed to ensure we can generate the same result each time we run these codes. we attach Auto dataset so that we can refer to the variables in this dataset without specifing them. Then we output the its dimention,variable names and the first 4 row of the dataset.  Hope you still remember these very basic codes. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">train&#x3D;sample(392,196)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>The we split the dataset into training dataset and testing dataset using the sample function, where we create a random sample of size 196 as training dataset from a population of size 392. Which accounts for 50% of the total data. And the sample function will output the index instead of the value.</p>
<pre class="line-numbers language-r" data-language="r"><code class="language-r">plot(horsepower,mpg)
#plot training set data
points(horsepower[train],mpg[train],col&#x3D;&quot;red&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>We are still analyzing the statistical relationship between horsepower and mile per gallon, so we can draw a scatter plot to show our original data and the splited  training data.  As you can see, we create a plot with horsepower as the x-axis and mpg as y-axis, the total dataset point is shown as circle black and the training dataset is circle red ones. </p>
<pre class="line-numbers language-r" data-language="r"><code class="language-r"># Define mean square error function
mse &lt;- function(y, y_pred) &#123;
  mean((y - y_pred)^2)
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>Before we fit the model, we can define the a function for caclulation of MSE so that we can use it easier. </p>
<p>We will use linear regression model, quadratic regression and cubic regression model and compare their donation in testing Mean square error. </p>
<pre class="line-numbers language-r" data-language="r"><code class="language-r"># Fit linear regression model and calculate mean squared error
lm.fit&#x3D;lm(mpg~horsepower,data&#x3D;Auto,subset&#x3D;train)
MSE &#x3D; mse(mpg[-train], predict(lm.fit,Auto)[-train])
MSE
abline(lm.fit, col&#x3D;&#39;black&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>We first conduct the linear regression model, we specify the subset equal to train for training. And use the mse function to calculate mean square error, as we want to calculate the test MSE to evaluate the effect of this model. So we need to use the test dataset of calculation. Thus, here, the negative sign is used to exclude the training index. so the rest index is the test dataset index. The output MSE is 25.2914, and we add the fitted curved to the plot with the color black. So this line is our linear regression fitted curve. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R"># Fit quadratic regression model and calculate mean squared error

lm.fit2&#x3D;lm(mpg~poly(horsepower,2),data&#x3D;Auto,subset&#x3D;train)
MSE &#x3D; mse(mpg[-train], predict(lm.fit2,Auto)[-train])
MSE
horselims &#x3D; range(horsepower) #返回horsepower里的最大值和最小值
horsepower.grid &#x3D; seq(from&#x3D;horselims[1],to&#x3D;horselims[2])# 创建等间隔的数列, horselims[1]是最小值
preds&#x3D;predict(lm.fit2,newdata&#x3D;list(horsepower&#x3D;horsepower.grid)) 
lines(horsepower.grid,preds,col&#x3D;&quot;green&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p> Then, repeat the process, we use the ploy function to conduct the cubic regression model, we calculate the MSE which is 20.  We also want to plot the fitted curve for this model, but we cannot use the abline to draw curve, cus abine can only be used to draw straight line not the curves. So Alternatively, we create some data to plot this curve. We can produce a equally-spaced series, from the minimum values of the predator variable to the maximun values of the predator variable, so that the line we plot can be drawn from the leftmost end of the x coordinate in the image through all the data up to the rightmost end.  We use the pred function to predict the estimated value of mpg based one the preditor value we create. So that we can plot the fitted curve in this plot. </p>
<pre class="line-numbers language-r" data-language="r"><code class="language-r"># Fit cubic regression model and calculate mean squared error
lm.fit3&#x3D;lm(mpg~poly(horsepower,3),data&#x3D;Auto,subset&#x3D;train)

MSE &#x3D; mse(mpg[-train], predict(lm.fit3,Auto)[-train])
MSE

preds&#x3D;predict(lm.fit3,newdata&#x3D;list(horsepower&#x3D;horsepower.grid))
lines(horsepower.grid,preds,col&#x3D;&quot;blue&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>We conduct the cubic regression model with the same process, just change two to three. The MSE is 20.02832 and the blue line is our cubic regression line. </p>
<p>So according to the validation set approach, we consider the cubic regression model is the best model can describe the realtionship of these two varivales becasue it has the lowest MSE. </p>
<p>However, the validation set approach has some limitations. First, it can result in high variance because the performance estimate depends heavily on which samples end up in the validation set. Second, it can be inefficient because a large portion of the data is not used for training. In this case, the training dataset only accounts for 50 percent. And if we split the dataset in different way, for example, if we change the seed to other values, our best estimated model changed from quardictic model to cubic regression model. </p>
<p>K-fold cross-validation is often preferred over the validation set approach because it provides a more reliable estimate of the model’s performance on new, unseen data.</p>
<p>cross-validation addresses these limitations by dividing the data into K non-overlapping folds, where K is usually between 5 and 10. The model is trained on K-1 folds and tested on the remaining fold, and this process is repeated K times. Each fold is used exactly once for testing, and the performance estimates from each fold are averaged to obtain the final estimate. This approach reduces the variance in the performance estimate because each sample is used for both training and testing. The drawback of this method is very obvious, it can be more computationally expensive, especially for large datasets and complex models because it will be repeated K times. </p>
<p>We can have a try of this approach. </p>
<pre class="line-numbers language-&#123;r&#125;" data-language="&#123;r&#125;"><code class="language-&#123;r&#125;">library(boot)
set.seed(1)
cv.error.10&#x3D;rep(0,10)
cv.error.10
for (i in 1:10)&#123;
  glm.fit&#x3D;glm(mpg~poly(horsepower,i),data&#x3D;Auto)     
  cv.error.10[i]&#x3D;cv.glm(Auto,glm.fit,K&#x3D;10)$delta[1] 
&#125;
cv.error.10
plot(1:10,cv.error.10,type&#x3D;&quot;b&quot;,col&#x3D;&quot;red&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>we want to conduct the 10-fold cross-validation, This line loads the <code>boot</code> package, which contains functions for performing bootstrap and cross-validation procedures. We set the seed for a random number and create a sequence with the length of 10, and their initial values for these sequence is zero, and we will update it with mean square error for each fold later.</p>
<p>This loop iterates from 1 to 10, and for each iteration it uisng the glm function to fit a polynomial regression model on the horsepower variables with of order ranging from 1 to 10. </p>
<p>Then, the cv.glm function is used to perform 10-fold cross-validation and obtain the cross-validation error for each order, which is saved in cv.error.10 array. And there are two elements in delta paramater, the first one is the Mean square error, the second one is standard deviation of error.  When evaluating models, we typically use the mean error (such as mean squared error or mean absolute error) rather than the standard deviation of the error. </p>
<p>This is because the mean error measures the average deviation between the predicted values and the actual values, while the standard deviation of the error measures the variability of the errors around their mean. The mean error provides a more straightforward and interpretable measure of model performance, as it tells us, on average, how far off the model’s predictions are from the actual values. The standard deviation of the error, on the other hand, is useful for understanding the spread of the errors and the overall uncertainty of the model’s predictions. So In this case, we want to store the Mean square error of each fold, so we specify the index of delta as 1. </p>
<p>Finally, this line plots the cross-validation error estimates for each polynomial order. The <code>type = &quot;b&quot;</code> argument specifies that both points and lines should be plotted, while the <code>col = &quot;red&quot;</code> argument sets the color of the points and lines to red. The x-axis displays the polynomial order, while the y-axis shows the mean squared error (MSE) of the cross-validation error estimates. So from the plot, by using the k fold cross validation, we can say the model with polynomial order 7 fit the model the best and can explain the data the best. </p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/05/06/Academic-Writing-Untitled-1/" title=""><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: </span></a><a class="button is-default" href="/2023/05/06/STAT318-Lab5/" title=""><span class="has-text-weight-semibold">Next: </span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article><article class="mt-6 comment-container" id="vcomments"></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/kathychen47"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Kathy Chen 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>