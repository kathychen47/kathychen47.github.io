<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Kathy Chen's blog</title><meta name="description" content="Kathy's learning notes"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Well, today, we are going to learn Decision Tress including classification tree and regression tree. I think you already know the principle of decision tree in class, so today we will learn how to implement them in code.
Classification treeelibrary(tree)
library(ISLR)
attach(Carseats)

We’re starting off by loading some libraries. Apart from the ISLR libra.."><script src="//unpkg.com/valine/dist/Valine.min.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Kathy Chen's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center"></p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Classification-treee"><span class="toc-text">Classification treee</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regression-tree"><span class="toc-text">Regression tree</span></a></li></ol></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle"></h1><time class="has-text-grey" datetime="2023-05-06T05:23:46.713Z">2023-05-06</time><article class="mt-2 post-content"><p>Well, today, we are going to learn Decision Tress including classification tree and regression tree. I think you already know the principle of decision tree in class, so today we will learn how to implement them in code.</p>
<h2 id="Classification-treee"><a href="#Classification-treee" class="headerlink" title="Classification treee"></a>Classification treee</h2><pre class="line-numbers language-r" data-language="r"><code class="language-r">library(tree)
library(ISLR)
attach(Carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>We’re starting off by loading some libraries. Apart from the ISLR library we used before, we will introduce a new library called tree, which we can use to build and visualize the decision tree. And we are going to use a new dataset: Carseats. Amd we attach it into R. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">str(Carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>First of all, we still need to get some information of this dataset, we print out the structure of this dataset, As it shown, Apart from Shelve Location, Urban and US, these three variables type are factor, others are all numeric variables.  </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">High&#x3D;ifelse(Sales&lt;&#x3D;8,&quot;No&quot;,&quot;Yes&quot;) #no 0 yes 1
Carseats1&#x3D;data.frame(Carseats,High)
Carseats1$High&#x3D;as.factor(Carseats1$High)
str(Carseats1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>Sales is the most important variable because our target is to predict the Sales based on all the other variables. In the regression tree, we want to estimate the number of sales, but in classification tree, we should  turn this dataset into a classification problem first.  So we’re creating new variable called High, if the number of car seat sold is lower than or equal to 8, we label it as NO, Otherwise, we label it as Yes. Then, we wanna add this new variable in the dataset, so we create a new data frame called Carseats1 which include all the variables in the original dataset and this new variable High. As you can see, the type of the High variable is character, so we should use as. factor function to transform it to factor type first, so that it can be used for classification. </p>
<pre class="line-numbers language-none"><code class="language-none">tree.carseats&#x3D;tree(High~.-Sales,Carseats1)
summary(tree.carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>After all these preparation, now, we can build a classification tree. The Tree function is used to build the classification tree, High is response variable, while other variables in the Carseats1 dataset except Sales are the predictor variables. We can print out a brief introduction of this model use the summary function. </p>
<p>First of all, it’s a classification tree, these are the variables actually used in tree construction.  There are 27 terminal nodes in this tree, that is, the model divides the dataset into 27 different categories or subsets. </p>
<p>Here is formula for calculate the residual mean deviance, in this formula, yi is …. So in this case, n is 400 cus there are 400 observations and T is 27. This value indicates that the model has a small average deviation between predicted and true values, indicating good model fit.  </p>
<p>The meaning of misclassification error is very obvious, 36 is the sample that we are false classified, this rate is only 0.09, which indicate a good model fit as well. </p>
<pre class="line-numbers language-r" data-language="r"><code class="language-r">plot(tree.carseats)
text(tree.carseats,pretty&#x3D;0,cex &#x3D; 0.6)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>Finally, we can visualize this tree by plotting the tree, and text() function is used to add the label in the plot, and, the <code>cex</code> parameter controls the size of the text labels. When your tree are very big, you can reduce the cex, so that the label will not overlap with each other. </p>
<p>The last line <code>tree.carseats</code> just prints the fitted decision tree model. This will show all the detail in the tree plot. This is this format. </p>
<p>The first element is the <code>node</code>, which is the node number, and if there are star signal at the end of it, it shows that this node is the terminal node. </p>
<p>The second element is <code>split</code>, which shows the predictor variable and the splitting threshold to partition the date at the node. </p>
<p><code>n</code> is the number of observations that reach the node</p>
<p>and <code>deviance</code> is the evaluation metric, for classification tree, it usually the Gini index or cross-entropy. For regression tree, it’s usually the residual sum of squares. </p>
<p><code>yval</code> is the predicted class. </p>
<p><code>yprob</code> is the class probabilities at the node. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">set.seed(1)
train&#x3D;sample(1:nrow(Carseats1), 200)
Carseats.test&#x3D;Carseats1[-train,]
High.test&#x3D;High[-train]
tree.carseats&#x3D;tree(High~.-Sales,Carseats1,subset&#x3D;train)
tree.pred&#x3D;predict(tree.carseats,Carseats.test,type&#x3D;&quot;class&quot;)
table(tree.pred,High.test)
(98+56)&#x2F;200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p>Even though we seem to have a good result, but it might be overfitting, so we need to evaluate this model to see how well our decision tree actually predict variable High. So we can use the evaluation method that we have learned in the previous class, The validation set approach. We split the dataset into training dataset and validation set. We’re setting a random seed for reproducibility, and then randomly selecting 200 rows from our dataset to use as our training data. We’re then using the remaining rows as our testing data. We’re fitting our decision tree using only the training data, and then using that tree to predict <code>High</code> for the testing data. The <code>table()</code> function helps us see how many of our predictions were correct. In this case, 98 + 56 of our predictions were correct, out of a total of 200 testing data points. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">set.seed(3)
cv.carseats&#x3D;cv.tree(tree.carseats,FUN&#x3D;prune.misclass)
cv.carseats

# oupput:
## $size
## [1] 20 18 10  8  6  4  2  1
## 
## $dev
## [1] 53 53 52 52 54 49 72 83
## 
## $k
## [1] -Inf  0.0  0.5  1.5  2.0  4.0 12.0 19.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>To better fit the model, we can use corss-validation to choose the tree complexity.  We can use cv. tree function to perform the cross validation. The first element inside is the mode we fit. And the FUN argument is to specify the function we want to used for pruning. In this case, we specify the pruning function as misclassification. We can also set it as gini index, cost-complexity or mean square error.</p>
<p>From the output of <code>cv.carseats</code>:</p>
<p><code>size</code> shows different size of decision trees, from the largest with 20 nodes to the smallest with one node</p>
<p><code>dev</code> shows the deviations of different size of the decision tree in cross-validation, which are the error rate. The smaller the deviation, the better the model. </p>
<p><code>k</code> is the pruning points of different size of decision trees. The pruning point is a numeric value that reflects the cost complexity between pruned and unpruned decision trees. The larger the pruning point, the simpler the pruned decision tree. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">plot(cv.carseats$size,cv.carseats$dev,type&#x3D;&quot;b&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>We can plot the size and deviation into a line graph, we can see that, the x-axis is the number of terminal nodes, and the y</p>
<p>axis is the error rate. When the size of decision tree is 4, the deviation reaches the minimum value. This means that this size of decision tree is optimal. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">prune.carseats&#x3D;prune.misclass(tree.carseats,best&#x3D;4)
plot(prune.carseats)
text(prune.carseats,pretty&#x3D;0)
tree.pred&#x3D;predict(prune.carseats,Carseats.test,type&#x3D;&quot;class&quot;)
table(tree.pred,High.test)
accuracy&#x3D;sum((tree.pred &#x3D;&#x3D; High.test))&#x2F;length(tree.pred)
accuracy

prune.carseats&#x3D;prune.misclass(tree.carseats,best&#x3D;6)
plot(prune.carseats)
text(prune.carseats,pretty&#x3D;0)
tree.pred&#x3D;predict(prune.carseats,Carseats.test,type&#x3D;&quot;class&quot;)
table(tree.pred,High.test)
accuracy&#x3D;sum((tree.pred &#x3D;&#x3D; High.test))&#x2F;length(tree.pred)
accuracy
summary(prune.carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Then, we can prune the tree, we first use the optimal size of decision decided by cross validation, which is 4, again we plot the tree, predict, and calculate the accuracy. We also set the best to 5 and compare their accuracy. As it shown, the accuracy for size 4 is actually lower than size of 5, it’s might be a little bit confusing, because the optimal model we get is clearly of size equal to 4. This is not uncommon and can happen due to randomness in the test set, the specific subset of data used for training and validation during cross-validation, or other factors. It is important to keep in mind that cross-validation provides an estimate of the model’s performance on unseen data, but the actual performance may differ on a particular test set. </p>
<p>You can see from the codes. When we perform the cross validation, the dataset we use is the total Carseats dataset, where the dataset will be split into k fold training set and validation set. But the dataset we used to calculate the accuracy is the test set. So, there is a certain amount of chance that the optimal model do not show the best effect in the test set. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">### Using CART to solve this problem
library(rpart)
fit &#x3D; rpart(High~.-Sales,method&#x3D;&quot;class&quot;,data&#x3D;Carseats1[train,])
# control&#x3D;rpart.control(minsplit&#x3D;2,cp&#x3D;0)) 
summary(fit)
plot(fit)
text(fit)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>The rpart() is also a function for building decision tree. Tree() and rpart() have some differences. As we have done for building a classification tree, we use tree()function to construct the classification tree, to avoid overfitting, we use cross validation to find the best size of the tree, and we prune the tree and we finally obtain the optimal classification tree model. </p>
<p>However, rpart() is an extension of the basic decision tree algorithm that. It include a build-in pruning method that can aviod overfitting and simplifying the tree structure. It also has build-in corss validation that can be used to decide the pruning parameter. </p>
<p>rpart() allows you to manually set up many parameters such as minsplit, cp, method. You can see all the parameters that can be set here. just use a question signal in front of the name of the package. </p>
<p>You can use plot function to draw the tree or use a specific plotting tool in rpart.tool package. As you can see, the graph plotted by rpart are more colorful. </p>
<p>you can also plot the complexity parpamter against the error of cross validation. The larger the cp parameter, the complex the tree, so we want to choose a simplest tree but not underfitting. </p>
<p>The red dashed line represents the minimum cross-validated error rate, and the vertical bars represent the 1-standard error rule, where the smallest value of CP within one standard error of the minimum is chosen as the optimal CP.</p>
<h2 id="Regression-tree"><a href="#Regression-tree" class="headerlink" title="Regression tree"></a>Regression tree</h2><pre class="line-numbers language-R" data-language="R"><code class="language-R">library(MASS)
set.seed(1)
train &#x3D; sample(1:nrow(Boston), nrow(Boston)&#x2F;2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>Apart from classification tree, we can also use tree function to fit the regression tree. This is am example, we use the Boston dataset, We split  the dataset, 50% will be used for training and testing respectively. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R"># Grow the regression tree
tree.boston&#x3D;tree(medv~.,Boston,subset&#x3D;train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Then, we can grow the regression tree, we use medv as response variable, and all the other variables as the predictor variables, we use subset train for training. </p>
<p>We can print out the summary of this model, just as the classification tree, it will output the variables actually used in the tree construction, the number of terminal nodes and the residual mean deviance and the distribution of residuals. </p>
<p>and we plot the tree, It is obvious that different from the classification tree, the estimated value of terminal nodes is not yes or no, its a numeric value. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R"># Use cross validation to choose tree complexity
cv.boston&#x3D;cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type&#x3D;&#39;b&#39;)
cv.boston<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>Then we can also use cross validation to choose tree complexity and prune the tree based on this. Again, we use cv.tree to perform the corss validation, and plot size of regression tree against deviation. As it shown, the regreesion tree with size 7 have the lowest deviation, which is our optimal model. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R">prune.boston&#x3D;prune.tree(tree.boston,best&#x3D;5)
plot(prune.boston)
text(prune.boston,pretty&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>After that, we can prune the tree, which the optimal size of 7. </p>
<pre class="line-numbers language-R" data-language="R"><code class="language-R"># Estimate the error of the tree
yhat&#x3D;predict(tree.boston,newdata&#x3D;Boston[-train,])
boston.test&#x3D;Boston[-train,&quot;medv&quot;]
mean((yhat-boston.test)^2)

yhat&#x3D;predict(prune.boston,newdata&#x3D;Boston[-train,])
boston.test&#x3D;Boston[-train,&quot;medv&quot;]
mean((yhat-boston.test)^2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>And it is a regression model, so we can use the mean square error to evaluate this model. We calculate the MSE before pruning and after pruning. </p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/05/06/STAT318-Lab4/" title=""><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: </span></a><a class="button is-default" href="/2023/04/06/Computed-Tomography-Recon-Filter-Back-Projection-FBP/" title="Filter Back Projection (FBP)"><span class="has-text-weight-semibold">Next: Filter Back Projection (FBP)</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article><article class="mt-6 comment-container" id="vcomments"></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/kathychen47"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Kathy Chen 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>