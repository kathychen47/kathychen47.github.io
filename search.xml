<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2023/05/06/Academic-Writing-Untitled-1/"/>
      <url>/2023/05/06/Academic-Writing-Untitled-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h2><p>Many prior studies have shown that… </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/05/06/STAT318-Lab4/"/>
      <url>/2023/05/06/STAT318-Lab4/</url>
      
        <content type="html"><![CDATA[<p>Good morning everyone, and welcome back to Lab class! I hope you all had a great break and are feeling refreshed and ready to dive into today’s lesson. Today, we’ll be covering the validation set approach, k-fold cross validation and Bootstrap method. These three method are both used to evaluate performance of the model. </p><p>I will introduce you the validation set approach first. For the validation set approach, the data is split into two parts: a training set and a validation set. The model is trained on the training set and the validation set is used to estimate its performance. </p><p> First of all, Let’s see what the dataset looks like, just like we do every time we get a new dataset. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">library(ISLR)set.seed(6678)data(Auto)attach(Auto)dim(Auto)names(Auto)Auto[1:4,]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We will continue using Auto dataset. We load the library ISLR for the use of Auto dataset and ggplots library for ploting. We set the seed to ensure we can generate the same result each time we run these codes. we attach Auto dataset so that we can refer to the variables in this dataset without specifing them. Then we output the its dimention,variable names and the first 4 row of the dataset.  Hope you still remember these very basic codes. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">train&#x3D;sample(392,196)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>The we split the dataset into training dataset and testing dataset using the sample function, where we create a random sample of size 196 as training dataset from a population of size 392. Which accounts for 50% of the total data. And the sample function will output the index instead of the value.</p><pre class="line-numbers language-r" data-language="r"><code class="language-r">plot(horsepower,mpg)#plot training set datapoints(horsepower[train],mpg[train],col&#x3D;&quot;red&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>We are still analyzing the statistical relationship between horsepower and mile per gallon, so we can draw a scatter plot to show our original data and the splited  training data.  As you can see, we create a plot with horsepower as the x-axis and mpg as y-axis, the total dataset point is shown as circle black and the training dataset is circle red ones. </p><pre class="line-numbers language-r" data-language="r"><code class="language-r"># Define mean square error functionmse &lt;- function(y, y_pred) &#123;  mean((y - y_pred)^2)&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Before we fit the model, we can define the a function for caclulation of MSE so that we can use it easier. </p><p>We will use linear regression model, quadratic regression and cubic regression model and compare their donation in testing Mean square error. </p><pre class="line-numbers language-r" data-language="r"><code class="language-r"># Fit linear regression model and calculate mean squared errorlm.fit&#x3D;lm(mpg~horsepower,data&#x3D;Auto,subset&#x3D;train)MSE &#x3D; mse(mpg[-train], predict(lm.fit,Auto)[-train])MSEabline(lm.fit, col&#x3D;&#39;black&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We first conduct the linear regression model, we specify the subset equal to train for training. And use the mse function to calculate mean square error, as we want to calculate the test MSE to evaluate the effect of this model. So we need to use the test dataset of calculation. Thus, here, the negative sign is used to exclude the training index. so the rest index is the test dataset index. The output MSE is 25.2914, and we add the fitted curved to the plot with the color black. So this line is our linear regression fitted curve. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R"># Fit quadratic regression model and calculate mean squared errorlm.fit2&#x3D;lm(mpg~poly(horsepower,2),data&#x3D;Auto,subset&#x3D;train)MSE &#x3D; mse(mpg[-train], predict(lm.fit2,Auto)[-train])MSEhorselims &#x3D; range(horsepower) #返回horsepower里的最大值和最小值horsepower.grid &#x3D; seq(from&#x3D;horselims[1],to&#x3D;horselims[2])# 创建等间隔的数列, horselims[1]是最小值preds&#x3D;predict(lm.fit2,newdata&#x3D;list(horsepower&#x3D;horsepower.grid)) lines(horsepower.grid,preds,col&#x3D;&quot;green&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> Then, repeat the process, we use the ploy function to conduct the cubic regression model, we calculate the MSE which is 20.  We also want to plot the fitted curve for this model, but we cannot use the abline to draw curve, cus abine can only be used to draw straight line not the curves. So Alternatively, we create some data to plot this curve. We can produce a equally-spaced series, from the minimum values of the predator variable to the maximun values of the predator variable, so that the line we plot can be drawn from the leftmost end of the x coordinate in the image through all the data up to the rightmost end.  We use the pred function to predict the estimated value of mpg based one the preditor value we create. So that we can plot the fitted curve in this plot. </p><pre class="line-numbers language-r" data-language="r"><code class="language-r"># Fit cubic regression model and calculate mean squared errorlm.fit3&#x3D;lm(mpg~poly(horsepower,3),data&#x3D;Auto,subset&#x3D;train)MSE &#x3D; mse(mpg[-train], predict(lm.fit3,Auto)[-train])MSEpreds&#x3D;predict(lm.fit3,newdata&#x3D;list(horsepower&#x3D;horsepower.grid))lines(horsepower.grid,preds,col&#x3D;&quot;blue&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We conduct the cubic regression model with the same process, just change two to three. The MSE is 20.02832 and the blue line is our cubic regression line. </p><p>So according to the validation set approach, we consider the cubic regression model is the best model can describe the realtionship of these two varivales becasue it has the lowest MSE. </p><p>However, the validation set approach has some limitations. First, it can result in high variance because the performance estimate depends heavily on which samples end up in the validation set. Second, it can be inefficient because a large portion of the data is not used for training. In this case, the training dataset only accounts for 50 percent. And if we split the dataset in different way, for example, if we change the seed to other values, our best estimated model changed from quardictic model to cubic regression model. </p><p>K-fold cross-validation is often preferred over the validation set approach because it provides a more reliable estimate of the model’s performance on new, unseen data.</p><p>cross-validation addresses these limitations by dividing the data into K non-overlapping folds, where K is usually between 5 and 10. The model is trained on K-1 folds and tested on the remaining fold, and this process is repeated K times. Each fold is used exactly once for testing, and the performance estimates from each fold are averaged to obtain the final estimate. This approach reduces the variance in the performance estimate because each sample is used for both training and testing. The drawback of this method is very obvious, it can be more computationally expensive, especially for large datasets and complex models because it will be repeated K times. </p><p>We can have a try of this approach. </p><pre class="line-numbers language-&#123;r&#125;" data-language="&#123;r&#125;"><code class="language-&#123;r&#125;">library(boot)set.seed(1)cv.error.10&#x3D;rep(0,10)cv.error.10for (i in 1:10)&#123;  glm.fit&#x3D;glm(mpg~poly(horsepower,i),data&#x3D;Auto)       cv.error.10[i]&#x3D;cv.glm(Auto,glm.fit,K&#x3D;10)$delta[1] &#125;cv.error.10plot(1:10,cv.error.10,type&#x3D;&quot;b&quot;,col&#x3D;&quot;red&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>we want to conduct the 10-fold cross-validation, This line loads the <code>boot</code> package, which contains functions for performing bootstrap and cross-validation procedures. We set the seed for a random number and create a sequence with the length of 10, and their initial values for these sequence is zero, and we will update it with mean square error for each fold later.</p><p>This loop iterates from 1 to 10, and for each iteration it uisng the glm function to fit a polynomial regression model on the horsepower variables with of order ranging from 1 to 10. </p><p>Then, the cv.glm function is used to perform 10-fold cross-validation and obtain the cross-validation error for each order, which is saved in cv.error.10 array. And there are two elements in delta paramater, the first one is the Mean square error, the second one is standard deviation of error.  When evaluating models, we typically use the mean error (such as mean squared error or mean absolute error) rather than the standard deviation of the error. </p><p>This is because the mean error measures the average deviation between the predicted values and the actual values, while the standard deviation of the error measures the variability of the errors around their mean. The mean error provides a more straightforward and interpretable measure of model performance, as it tells us, on average, how far off the model’s predictions are from the actual values. The standard deviation of the error, on the other hand, is useful for understanding the spread of the errors and the overall uncertainty of the model’s predictions. So In this case, we want to store the Mean square error of each fold, so we specify the index of delta as 1. </p><p>Finally, this line plots the cross-validation error estimates for each polynomial order. The <code>type = &quot;b&quot;</code> argument specifies that both points and lines should be plotted, while the <code>col = &quot;red&quot;</code> argument sets the color of the points and lines to red. The x-axis displays the polynomial order, while the y-axis shows the mean squared error (MSE) of the cross-validation error estimates. So from the plot, by using the k fold cross validation, we can say the model with polynomial order 7 fit the model the best and can explain the data the best. </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/05/06/STAT318-Lab5/"/>
      <url>/2023/05/06/STAT318-Lab5/</url>
      
        <content type="html"><![CDATA[<p>Well, today, we are going to learn Decision Tress including classification tree and regression tree. I think you already know the principle of decision tree in class, so today we will learn how to implement them in code.</p><h2 id="Classification-treee"><a href="#Classification-treee" class="headerlink" title="Classification treee"></a>Classification treee</h2><pre class="line-numbers language-r" data-language="r"><code class="language-r">library(tree)library(ISLR)attach(Carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>We’re starting off by loading some libraries. Apart from the ISLR library we used before, we will introduce a new library called tree, which we can use to build and visualize the decision tree. And we are going to use a new dataset: Carseats. Amd we attach it into R. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">str(Carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>First of all, we still need to get some information of this dataset, we print out the structure of this dataset, As it shown, Apart from Shelve Location, Urban and US, these three variables type are factor, others are all numeric variables.  </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">High&#x3D;ifelse(Sales&lt;&#x3D;8,&quot;No&quot;,&quot;Yes&quot;) #no 0 yes 1Carseats1&#x3D;data.frame(Carseats,High)Carseats1$High&#x3D;as.factor(Carseats1$High)str(Carseats1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Sales is the most important variable because our target is to predict the Sales based on all the other variables. In the regression tree, we want to estimate the number of sales, but in classification tree, we should  turn this dataset into a classification problem first.  So we’re creating new variable called High, if the number of car seat sold is lower than or equal to 8, we label it as NO, Otherwise, we label it as Yes. Then, we wanna add this new variable in the dataset, so we create a new data frame called Carseats1 which include all the variables in the original dataset and this new variable High. As you can see, the type of the High variable is character, so we should use as. factor function to transform it to factor type first, so that it can be used for classification. </p><pre class="line-numbers language-none"><code class="language-none">tree.carseats&#x3D;tree(High~.-Sales,Carseats1)summary(tree.carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>After all these preparation, now, we can build a classification tree. The Tree function is used to build the classification tree, High is response variable, while other variables in the Carseats1 dataset except Sales are the predictor variables. We can print out a brief introduction of this model use the summary function. </p><p>First of all, it’s a classification tree, these are the variables actually used in tree construction.  There are 27 terminal nodes in this tree, that is, the model divides the dataset into 27 different categories or subsets. </p><p>Here is formula for calculate the residual mean deviance, in this formula, yi is …. So in this case, n is 400 cus there are 400 observations and T is 27. This value indicates that the model has a small average deviation between predicted and true values, indicating good model fit.  </p><p>The meaning of misclassification error is very obvious, 36 is the sample that we are false classified, this rate is only 0.09, which indicate a good model fit as well. </p><pre class="line-numbers language-r" data-language="r"><code class="language-r">plot(tree.carseats)text(tree.carseats,pretty&#x3D;0,cex &#x3D; 0.6)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Finally, we can visualize this tree by plotting the tree, and text() function is used to add the label in the plot, and, the <code>cex</code> parameter controls the size of the text labels. When your tree are very big, you can reduce the cex, so that the label will not overlap with each other. </p><p>The last line <code>tree.carseats</code> just prints the fitted decision tree model. This will show all the detail in the tree plot. This is this format. </p><p>The first element is the <code>node</code>, which is the node number, and if there are star signal at the end of it, it shows that this node is the terminal node. </p><p>The second element is <code>split</code>, which shows the predictor variable and the splitting threshold to partition the date at the node. </p><p><code>n</code> is the number of observations that reach the node</p><p>and <code>deviance</code> is the evaluation metric, for classification tree, it usually the Gini index or cross-entropy. For regression tree, it’s usually the residual sum of squares. </p><p><code>yval</code> is the predicted class. </p><p><code>yprob</code> is the class probabilities at the node. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">set.seed(1)train&#x3D;sample(1:nrow(Carseats1), 200)Carseats.test&#x3D;Carseats1[-train,]High.test&#x3D;High[-train]tree.carseats&#x3D;tree(High~.-Sales,Carseats1,subset&#x3D;train)tree.pred&#x3D;predict(tree.carseats,Carseats.test,type&#x3D;&quot;class&quot;)table(tree.pred,High.test)(98+56)&#x2F;200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Even though we seem to have a good result, but it might be overfitting, so we need to evaluate this model to see how well our decision tree actually predict variable High. So we can use the evaluation method that we have learned in the previous class, The validation set approach. We split the dataset into training dataset and validation set. We’re setting a random seed for reproducibility, and then randomly selecting 200 rows from our dataset to use as our training data. We’re then using the remaining rows as our testing data. We’re fitting our decision tree using only the training data, and then using that tree to predict <code>High</code> for the testing data. The <code>table()</code> function helps us see how many of our predictions were correct. In this case, 98 + 56 of our predictions were correct, out of a total of 200 testing data points. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">set.seed(3)cv.carseats&#x3D;cv.tree(tree.carseats,FUN&#x3D;prune.misclass)cv.carseats# oupput:## $size## [1] 20 18 10  8  6  4  2  1## ## $dev## [1] 53 53 52 52 54 49 72 83## ## $k## [1] -Inf  0.0  0.5  1.5  2.0  4.0 12.0 19.0## ## $method## [1] &quot;misclass&quot;## ## attr(,&quot;class&quot;)## [1] &quot;prune&quot;         &quot;tree.sequence&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>To better fit the model, we can use corss-validation to choose the tree complexity.  We can use cv. tree function to perform the cross validation. The first element inside is the mode we fit. And the FUN argument is to specify the function we want to used for pruning. In this case, we specify the pruning function as misclassification. We can also set it as gini index, cost-complexity or mean square error.</p><p>From the output of <code>cv.carseats</code>:</p><p><code>size</code> shows different size of decision trees, from the largest with 20 nodes to the smallest with one node</p><p><code>dev</code> shows the deviations of different size of the decision tree in cross-validation, which are the error rate. The smaller the deviation, the better the model. </p><p><code>k</code> is the pruning points of different size of decision trees. The pruning point is a numeric value that reflects the cost complexity between pruned and unpruned decision trees. The larger the pruning point, the simpler the pruned decision tree. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">plot(cv.carseats$size,cv.carseats$dev,type&#x3D;&quot;b&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>We can plot the size and deviation into a line graph, we can see that, the x-axis is the number of terminal nodes, and the y</p><p>axis is the error rate. When the size of decision tree is 4, the deviation reaches the minimum value. This means that this size of decision tree is optimal. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">prune.carseats&#x3D;prune.misclass(tree.carseats,best&#x3D;4)plot(prune.carseats)text(prune.carseats,pretty&#x3D;0)tree.pred&#x3D;predict(prune.carseats,Carseats.test,type&#x3D;&quot;class&quot;)table(tree.pred,High.test)accuracy&#x3D;sum((tree.pred &#x3D;&#x3D; High.test))&#x2F;length(tree.pred)accuracyprune.carseats&#x3D;prune.misclass(tree.carseats,best&#x3D;6)plot(prune.carseats)text(prune.carseats,pretty&#x3D;0)tree.pred&#x3D;predict(prune.carseats,Carseats.test,type&#x3D;&quot;class&quot;)table(tree.pred,High.test)accuracy&#x3D;sum((tree.pred &#x3D;&#x3D; High.test))&#x2F;length(tree.pred)accuracysummary(prune.carseats)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Then, we can prune the tree, we first use the optimal size of decision decided by cross validation, which is 4, again we plot the tree, predict, and calculate the accuracy. We also set the best to 5 and compare their accuracy. As it shown, the accuracy for size 4 is actually lower than size of 5, it’s might be a little bit confusing, because the optimal model we get is clearly of size equal to 4. This is not uncommon and can happen due to randomness in the test set, the specific subset of data used for training and validation during cross-validation, or other factors. It is important to keep in mind that cross-validation provides an estimate of the model’s performance on unseen data, but the actual performance may differ on a particular test set. </p><p>You can see from the codes. When we perform the cross validation, the dataset we use is the total Carseats dataset, where the dataset will be split into k fold training set and validation set. But the dataset we used to calculate the accuracy is the test set. So, there is a certain amount of chance that the optimal model do not show the best effect in the test set. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">### Using CART to solve this problemlibrary(rpart)fit &#x3D; rpart(High~.-Sales,method&#x3D;&quot;class&quot;,data&#x3D;Carseats1[train,])# control&#x3D;rpart.control(minsplit&#x3D;2,cp&#x3D;0)) summary(fit)plot(fit)text(fit)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The rpart() is also a function for building decision tree. Tree() and rpart() have some differences. As we have done for building a classification tree, we use tree()function to construct the classification tree, to avoid overfitting, we use cross validation to find the best size of the tree, and we prune the tree and we finally obtain the optimal classification tree model. </p><p>However, rpart() is an extension of the basic decision tree algorithm that. It include a build-in pruning method that can aviod overfitting and simplifying the tree structure. It also has build-in corss validation that can be used to decide the pruning parameter. </p><p>rpart() allows you to manually set up many parameters such as minsplit, cp, method. You can see all the parameters that can be set here. just use a question signal in front of the name of the package. </p><p>You can use plot function to draw the tree or use a specific plotting tool in rpart.tool package. As you can see, the graph plotted by rpart are more colorful. </p><p>you can also plot the complexity parpamter against the error of cross validation. The larger the cp parameter, the complex the tree, so we want to choose a simplest tree but not underfitting. </p><p>The red dashed line represents the minimum cross-validated error rate, and the vertical bars represent the 1-standard error rule, where the smallest value of CP within one standard error of the minimum is chosen as the optimal CP.</p><h2 id="Regression-tree"><a href="#Regression-tree" class="headerlink" title="Regression tree"></a>Regression tree</h2><pre class="line-numbers language-R" data-language="R"><code class="language-R">library(MASS)set.seed(1)train &#x3D; sample(1:nrow(Boston), nrow(Boston)&#x2F;2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>Apart from classification tree, we can also use tree function to fit the regression tree. This is am example, we use the Boston dataset, We split  the dataset, 50% will be used for training and testing respectively. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R"># Grow the regression treetree.boston&#x3D;tree(medv~.,Boston,subset&#x3D;train)summary(tree.boston)plot(tree.boston)text(tree.boston,pretty&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Then, we can grow the regression tree, we use medv as response variable, and all the other variables as the predictor variables, we use subset train for training. </p><p>We can print out the summary of this model, just as the classification tree, it will output the variables actually used in the tree construction, the number of terminal nodes and the residual mean deviance and the distribution of residuals. </p><p>and we plot the tree, It is obvious that different from the classification tree, the estimated value of terminal nodes is not yes or no, its a numeric value. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R"># Use cross validation to choose tree complexitycv.boston&#x3D;cv.tree(tree.boston)plot(cv.boston$size,cv.boston$dev,type&#x3D;&#39;b&#39;)cv.boston<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Then we can also use cross validation to choose tree complexity and prune the tree based on this. Again, we use cv.tree to perform the corss validation, and plot size of regression tree against deviation. As it shown, the regreesion tree with size 7 have the lowest deviation, which is our optimal model. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R">prune.boston&#x3D;prune.tree(tree.boston,best&#x3D;5)plot(prune.boston)text(prune.boston,pretty&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>After that, we can prune the tree, which the optimal size of 7. </p><pre class="line-numbers language-R" data-language="R"><code class="language-R"># Estimate the error of the treeyhat&#x3D;predict(tree.boston,newdata&#x3D;Boston[-train,])boston.test&#x3D;Boston[-train,&quot;medv&quot;]mean((yhat-boston.test)^2)yhat&#x3D;predict(prune.boston,newdata&#x3D;Boston[-train,])boston.test&#x3D;Boston[-train,&quot;medv&quot;]mean((yhat-boston.test)^2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>And it is a regression model, so we can use the mean square error to evaluate this model. We calculate the MSE before pruning and after pruning. </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Filter Back Projection (FBP)</title>
      <link href="/2023/04/06/Computed-Tomography-Recon-Filter-Back-Projection-FBP/"/>
      <url>/2023/04/06/Computed-Tomography-Recon-Filter-Back-Projection-FBP/</url>
      
        <content type="html"><![CDATA[<h2 id="Principle"><a href="#Principle" class="headerlink" title="Principle"></a>Principle</h2><p>It states that the one-dimensional Fourier transformer of the projection of a function or <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="11.266ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4979.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1223,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(2229,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(2695,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3084,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3656,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4100.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4590.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> along a direction <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container>, is equal to a vertical slice two dimensional Fourier transform <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.251ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4088.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(749,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1138,0)"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mn" transform="translate(655,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2196.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2641.2,0)"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mn" transform="translate(655,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3699.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.413ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2834.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1511,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1955.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2445.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> passing through the origin and oriented at an angle <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container> with respect to the</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.395ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1058.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mn" transform="translate(655,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container></p><p>axis.</p><p>Assume <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.631ex" height="1.441ex" role="img" focusable="false" viewBox="0 -626 1163 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mtext" transform="translate(361,0)"><path data-c="2D" d="M11 179V252H277V179H11Z"></path></g><g data-mml-node="mi" transform="translate(694,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container> is a rotated coordinate system according to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.156ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 1395 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mtext" transform="translate(572,0)"><path data-c="2D" d="M11 179V252H277V179H11Z"></path></g><g data-mml-node="mi" transform="translate(905,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>.</p><p>Slice <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.413ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2834.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1511,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1955.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2445.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> in <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.156ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 1395 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mtext" transform="translate(572,0)"><path data-c="2D" d="M11 179V252H277V179H11Z"></path></g><g data-mml-node="mi" transform="translate(905,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> coordinate is shown as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.636ex" height="2.283ex" role="img" focusable="false" viewBox="0 -759 2933.1 1009"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(636,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path></g></g></g><g data-mml-node="mo" transform="translate(880.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1269.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1630.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2075.1,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(2544.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> in <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.631ex" height="1.441ex" role="img" focusable="false" viewBox="0 -626 1163 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mtext" transform="translate(361,0)"><path data-c="2D" d="M11 179V252H277V179H11Z"></path></g><g data-mml-node="mi" transform="translate(694,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container> coordinate system.</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.159ex;" xmlns="http://www.w3.org/2000/svg" width="38.825ex" height="5.553ex" role="img" focusable="false" viewBox="0 -1500.3 17160.8 2454.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(636,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path></g></g></g><g data-mml-node="mo" transform="translate(880.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1269.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1630.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2075.1,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(2544.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3210.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msubsup" transform="translate(4266.7,0)"><g data-mml-node="mo" transform="translate(0 1)"><path data-c="222B" d="M114 -798Q132 -824 165 -824H167Q195 -824 223 -764T275 -600T320 -391T362 -164Q365 -143 367 -133Q439 292 523 655T645 1127Q651 1145 655 1157T672 1201T699 1257T733 1306T777 1346T828 1360Q884 1360 912 1325T944 1245Q944 1220 932 1205T909 1186T887 1183Q866 1183 849 1198T832 1239Q832 1287 885 1296L882 1300Q879 1303 874 1307T866 1313Q851 1323 833 1323Q819 1323 807 1311T775 1255T736 1139T689 936T633 628Q574 293 510 -5T410 -437T355 -629Q278 -862 165 -862Q125 -862 92 -831T55 -746Q55 -711 74 -698T112 -685Q133 -685 150 -700T167 -741Q167 -789 114 -798Z"></path></g><g data-mml-node="mi" transform="translate(1046.4,1088.1) scale(0.707)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mn" transform="translate(589,-896.4) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5932.8,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="msubsup" transform="translate(7088.5,0)"><g data-mml-node="mo" transform="translate(0 1)"><path data-c="222B" d="M114 -798Q132 -824 165 -824H167Q195 -824 223 -764T275 -600T320 -391T362 -164Q365 -143 367 -133Q439 292 523 655T645 1127Q651 1145 655 1157T672 1201T699 1257T733 1306T777 1346T828 1360Q884 1360 912 1325T944 1245Q944 1220 932 1205T909 1186T887 1183Q866 1183 849 1198T832 1239Q832 1287 885 1296L882 1300Q879 1303 874 1307T866 1313Q851 1323 833 1323Q819 1323 807 1311T775 1255T736 1139T689 936T633 628Q574 293 510 -5T410 -437T355 -629Q278 -862 165 -862Q125 -862 92 -831T55 -746Q55 -711 74 -698T112 -685Q133 -685 150 -700T167 -741Q167 -789 114 -798Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1046.4,1088.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(589,-896.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9608.8,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mo" transform="translate(1762,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2206.7,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2675.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mrow" transform="translate(3231.3,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mo" transform="translate(900,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(14018.1,0)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mi" transform="translate(1692,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(16018.8,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(16538.8,0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g></g></g></svg></mjx-container></p><h2 id="Code-In-Python"><a href="#Code-In-Python" class="headerlink" title="Code In Python"></a>Code In Python</h2><ol><li>fbp.py</li></ol><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">import pydicomfrom PIL import Image, ImageOpsimport matplotlib.pyplot as pltimport numpy as npfrom scipy.fftpack import fft, ifft, fftshiftimport fbpdef arange2(start, stop=None, step=1):    """#Modified version of numpy.arange which corrects error associated with non-integer step size"""    if stop == None:        a = np.arange(start)    else:        a = np.arange(start, stop, step)        if a[-1] &gt; stop-step:            a = np.delete(a, -1)    return adef getProj(img,theta):    numAngles = len(theta)    sinogram = np.zeros((img.size[0], numAngles))    # Iteratively calculate the integral value of each projection    for n in range(numAngles):        theta = np.linspace(0., 180., numAngles, endpoint=False)        # rotate the image        rotImgObj = img.rotate(90-theta[n], resample=Image.BICUBIC)        # calculate the integral value of porjection        sinogram[:,n] = np.sum(rotImgObj, axis=0)    return sinogramdef filterProj(sinogram):    projLen, numAngles = sinogram.shape    step = 2 * np.pi / projLen    w = fbp.arange2(-np.pi, np.pi, step)    if len(w)&lt;projLen:        w = np.concatenate([w, [w[-1] + step]])    ramp_filter = np.abs(w)    ramp_filter = fftshift(ramp_filter)  # shifts the zero-frequency component of the discrete Fourier transform (DFT) to the center of the arraydef filterProj2(sinogram,a=0.1):    projLen, numAngles = sinogram.shape    step = 2 * np.pi / projLen    w = arange2(-np.pi, np.pi, step)    if len(w) &lt; projLen:        w = np.concatenate([w, [w[-1] + step]])  # depending on image size, it might be that len(w) =        # projLen - 1. Another element is added to w in this case    rn1 = abs(2 / a * np.sin(a * w / 2));  # approximation of ramp filter abs(w) with a funciton abs(sin(w))    rn2 = np.sin(a * w / 2) / (a * w / 2);  # sinc window with 'a' modifying the cutoff freqs    ramp_filter = rn1 * (rn2) ** 2;    ramp_filter = fftshift(ramp_filter)    filter_sinogram = np.zeros_like(sinogram)    for i in range(numAngles):        projfft = fft(sinogram[:, i])  # one dimentional Fourier transform        filter_projfft = projfft * ramp_filter  # *ramp_filter        filter_proj = np.real(ifft(filter_projfft))  # inverse Fourier transform        filter_sinogram[:, i] = filter_proj    plt.imshow(filter_sinogram, cmap='gray')    plt.title("filter_sinogram")    plt.show()    return filter_sinogramdef backPorj(filter_sinogram, theta):    # define a empty reconstruction image    N = filter_sinogram.shape[0]    reconImg = np.zeros((N, N))  # the length of image is equal to the length of detector    # Create a grid coordinate system with the center point at (0,0) in which the origin of the image coordinate system,    # which is located at the upper left corner, is shifted to the center point.    X = fbp.arange2(N) - N / 2  # [-N/2,...0..+N/2]    Y = X.copy()    x, y = np.meshgrid(X, Y)  # generate grid coordinate    # Convert degrees to radians    theta = theta * np.pi / 180    numAngles = len(theta)    for n in range(numAngles):        t = x * np.sin(theta[n]) + y * np.cos(theta[n])  # x-y -&gt; t-s        # s = -x*np.sin(theta[n])+y*np.cos(theta[n])        tCor = np.round(            t + N / 2)  # Shift the coordinate axis origin to match the image origin. round() may result in floating-point numbers, needs to be rounded to an integer.        tCor = tCor.astype("int")        tIndex, sIndex = np.where((tCor &gt;= 0) &amp; (tCor &lt;= N - 1))  # tIndex: row index; sIndex: column Index        sino_angle_n = filter_sinogram[:, n]        reconImg[tIndex, sIndex] += sino_angle_n[tCor[tIndex, sIndex]]        # plt.imshow(reconImg, cmap='gray')        # plt.title('Image {}'.format(n))        # plt.show()    reconImg = Image.fromarray((reconImg - np.min(reconImg)) / np.ptp(reconImg) * 255)  # normilization    fig = plt.imshow(reconImg)    plt.title('reconImg')    plt.show()    return reconImgdef lineProfile(height, img, reconImg):    line_profile = np.array(img)[height, :]    recon_line_profile = np.array(reconImg)[height, :]    plt.plot(line_profile, label='Original Image')    plt.plot(recon_line_profile, label='Reconstructed Image')    plt.xlabel('Pixel Value')    plt.ylabel('Intensity')    plt.title('Line Profile at Pixel height= {}'.format(height))    plt.legend()    plt.show()    return line_profile, recon_line_profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>main.py</li></ol><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">import fbpfrom PIL import Image, ImageChops, ImageOpsimport pydicomimport matplotlib.pyplot as pltimport numpy as npfrom scipy.fftpack import fft, ifft, fftshiftimport loggingimport pickleimport jsonif __name__ == '__main__':    # generate output logging    logging.basicConfig(filename='output.log', level=logging.DEBUG)    logger = logging.getLogger(__name__)    # --------------------------------------------------Load data----------------------------------------------------------    dcm = pydicom.dcmread('C:\\Users\\Kathy\\Desktop\\DL\FBP\\1.2.826.0.1.3680043.5876\\1.dcm')    img = Image.fromarray(dcm.pixel_array).convert('L')    # img = np.asarray(img)    # img = (img - np.min(img)) / np.ptp(img) * 255    # img = img.astype(np.uint8)    # img = Image.fromarray(img)    logger.debug(f"Original_img: {img}")    plt.imshow(img, cmap='gray')    plt.title("original_image")    plt.show()    # -----------------------------------------------Forward Projection-----------------------------------------------------    # Define rotation angle and the number of rotation    numAngles = 720    theta = np.linspace(0., 180., numAngles, endpoint=False)    sinogram = fbp.getProj(img, theta)    logger.debug(f"sinogram: {sinogram}")    # -----------------------------------------------sinogram filteration---------------------------------------------------    filter_sinogram=fbp.filterProj2(sinogram)    logger.debug(f"filter_sinogram: {filter_sinogram}")    # --------------------------------------------- Back Projection/ Recon -------------------------------------------------    reconImg = fbp.backPorj(filter_sinogram, theta)    logger.debug(f"reconImg: {reconImg}")    # generate lineProfile    fbp.lineProfile(200,img,reconImg)    # Load logging    with open('output.log', 'r') as f:        content = f.readlines()        print(content)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Computed Tomography </category>
          
          <category> Reconstruction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FBP </tag>
            
            <tag> Filter Back Projection </tag>
            
            <tag> CT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Iterative reconstruction (IR)</title>
      <link href="/2023/04/06/Computed-Tomography-Recon-Iterative-Reconstruction-IR/"/>
      <url>/2023/04/06/Computed-Tomography-Recon-Iterative-Reconstruction-IR/</url>
      
        <content type="html"><![CDATA[<p>Algebraic reconstruction technique includes:</p><ol><li><p>Algebraic Iterative Reconstruction</p><ul><li>Algebraic Reconstruction technique (ART)</li><li>Simultaneous Algebraic Reconstruction Technique (SART)</li><li>Maximum Likelihood Expectation Maximum (MLEM)</li></ul></li><li><p>Statistical Iterative Reconstruction</p><ul><li>Ordered Subsets Expectation Maximization (OSEM)</li></ul></li></ol><h2 id="Algebraic-Reconstruction-Techniques-ART"><a href="#Algebraic-Reconstruction-Techniques-ART" class="headerlink" title="Algebraic Reconstruction Techniques (ART)"></a>Algebraic Reconstruction Techniques (ART)</h2><p>The problem of Iterative reconstruction problem can be reduced to solving the following linear equation: </p>]]></content>
      
      
      <categories>
          
          <category> Computed Tomography </category>
          
          <category> Reconstruction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IR </tag>
            
            <tag> Iterative reconstruction </tag>
            
            <tag> ART </tag>
            
            <tag> SART </tag>
            
            <tag> MLEM </tag>
            
            <tag> OSEM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Download Kaggle dataset in Linux server</title>
      <link href="/2023/02/28/Software-Setting-Kaggle-How-to-Download-Kaggle-dataset-in-Linux-server/"/>
      <url>/2023/02/28/Software-Setting-Kaggle-How-to-Download-Kaggle-dataset-in-Linux-server/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Install-Kaggle-in-Linux"><a href="#1-Install-Kaggle-in-Linux" class="headerlink" title="1. Install Kaggle in Linux"></a>1. Install Kaggle in Linux</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install kaggle<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-Get-Kaggle-API"><a href="#2-Get-Kaggle-API" class="headerlink" title="2. Get Kaggle API"></a>2. Get Kaggle API</h2><p>Kaggle -&gt; Account -&gt; Create New API Token -&gt; download kaggle.json -&gt; move kaggle. json file to home/user/.kaggle in linux</p><h2 id="3-Change-Download-Directory"><a href="#3-Change-Download-Directory" class="headerlink" title="3. Change Download Directory"></a>3. Change Download Directory</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">kaggle config set -n path -v /data/XRay_data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="4-Download-Dataset"><a href="#4-Download-Dataset" class="headerlink" title="4. Download Dataset"></a>4. Download Dataset</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Software Setting </category>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
